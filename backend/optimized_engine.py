import os
import re
import pandas as pd
import requests
import json
import time
from typing import List, Dict
from dotenv import load_dotenv
from collections import Counter, defaultdict
import sqlite3
import hashlib

load_dotenv()

class OptimizedSEOAgent:
    def __init__(self):
        self.nim_client = NVIDIAQwenClient()
        self.serpapi_key = os.getenv('SERPAPI_API_KEY')
        self.setup_cache()
        self.request_count = 0
        self.last_request_time = 0
        
    def setup_cache(self):
        """Setup SQLite cache for API responses"""
        self.conn = sqlite3.connect('keyword_cache.db', check_same_thread=False)
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS keyword_cache (
                key_hash TEXT PRIMARY KEY,
                keyword TEXT,
                response_data TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        self.conn.commit()
    
    def get_cache_key(self, keyword: str, endpoint: str) -> str:
        """Generate cache key"""
        return hashlib.md5(f"{keyword}_{endpoint}".encode()).hexdigest()
    
    def get_cached_response(self, cache_key: str):
        """Get cached response"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT response_data FROM keyword_cache WHERE key_hash = ?', (cache_key,))
        result = cursor.fetchone()
        return json.loads(result[0]) if result else None
    
    def cache_response(self, cache_key: str, keyword: str, data: dict):
        """Cache API response"""
        cursor = self.conn.cursor()
        cursor.execute(
            'INSERT OR REPLACE INTO keyword_cache (key_hash, keyword, response_data) VALUES (?, ?, ?)',
            (cache_key, keyword, json.dumps(data))
        )
        self.conn.commit()
    
    def rate_limit(self):
        """Implement rate limiting for free APIs"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        # Free tier friendly: 2 requests per second max
        if time_since_last < 0.5:
            time.sleep(0.5 - time_since_last)
        
        self.last_request_time = time.time()
        self.request_count += 1

class NVIDIAQwenClient:
    def __init__(self):
        self.api_key = os.getenv('NVIDIA_API_KEY')
        self.base_url = os.getenv('NVIDIA_BASE_URL')
        # Using a model that's available in free tier
        self.model = "qwen/qwen-2.5-72b-instruct"
        
    def generate_keywords(self, seed_keyword: str, num_candidates: int = 80) -> List[str]:
        """Generate keywords with optimized prompt for better results"""
        
        prompt = self._create_optimized_prompt(seed_keyword, num_candidates)
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'model': self.model,
            'messages': [
                {
                    'role': 'system',
                    'content': 'You are an SEO expert. Return ONLY valid JSON arrays. No explanations.'
                },
                {
                    'role': 'user', 
                    'content': prompt
                }
            ],
            'temperature': 0.8,  # Slightly higher for diversity
            'max_tokens': 1500,  # Optimized for cost
            'stream': False
        }
        
        try:
            response = requests.post(
                f'{self.base_url}/chat/completions',
                headers=headers,
                json=payload,
                timeout=25
            )
            
            if response.status_code == 200:
                result = response.json()
                keywords_text = result['choices'][0]['message']['content'].strip()
                return self._parse_keywords_response(keywords_text, num_candidates)
            else:
                print(f"âš ï¸ NVIDIA API returned {response.status_code}. Using fallback.")
                return self._fallback_expansion(seed_keyword)
                
        except Exception as e:
            print(f"âš ï¸ NVIDIA request failed: {e}. Using fallback.")
            return self._fallback_expansion(seed_keyword)
    
    def _create_optimized_prompt(self, seed_keyword: str, num_candidates: int) -> str:
        """Create optimized prompt for better keyword generation"""
        return f"""Generate exactly {num_candidates} diverse SEO keyword variations for "{seed_keyword}".

CRITICAL: Return ONLY a valid JSON array. No other text.

Requirements:
- Mix of short-tail and long-tail keywords
- Include question formats (how, what, why)
- Include commercial intent keywords (buy, price, review)
- Include informational intent (guide, tutorial, tips)
- Include local intent (near me, in [city])
- Include comparison keywords (vs, alternative)

Example format: ["keyword1", "keyword2", "keyword3"]"""
    
    def _parse_keywords_response(self, response_text: str, expected_count: int) -> List[str]:
        """Parse and validate the keywords response"""
        try:
            # Clean the response
            cleaned = re.sub(r'^```json\s*|\s*```$', '', response_text.strip())
            keywords = json.loads(cleaned)
            
            if isinstance(keywords, list) and all(isinstance(k, str) for k in keywords):
                # Ensure we have enough keywords
                if len(keywords) >= expected_count:
                    return keywords[:expected_count]
                else:
                    # If we got fewer than expected, supplement with fallback
                    base_keywords = keywords
                    supplemented = self._fallback_expansion(" ".join(keywords[:2]) if keywords else "keyword")
                    return (base_keywords + supplemented)[:expected_count]
            else:
                raise ValueError("Invalid response format")
                
        except Exception as e:
            print(f"âš ï¸ Failed to parse keywords: {e}. Using fallback.")
            return self._fallback_expansion("keyword")
    
    def _fallback_expansion(self, seed_keyword: str) -> List[str]:
        """Enhanced fallback keyword expansion"""
        words = seed_keyword.lower().split()
        base_phrase = " ".join(words)
        
        # Comprehensive modifier sets
        prefixes = ['best', 'top', 'quality', 'professional', 'advanced', 'complete']
        suffixes = ['guide', 'tutorial', 'course', 'training', 'lessons', 'examples']
        questions = ['how to', 'what is', 'why use', 'when to use', 'where to find']
        commercial = ['buy', 'price', 'cost', 'review', 'comparison', 'vs', 'alternative']
        informational = ['guide', 'tutorial', 'tips', 'examples', 'basics', 'advanced']
        local = ['near me', 'online', 'free', '2024', '2025']
        
        variations = [base_phrase]
        
        # Generate combinations efficiently
        for prefix in prefixes:
            variations.append(f"{prefix} {base_phrase}")
        
        for suffix in suffixes:
            variations.append(f"{base_phrase} {suffix}")
        
        for question in questions:
            variations.append(f"{question} {base_phrase}")
        
        for comm in commercial:
            variations.append(f"{base_phrase} {comm}")
        
        # Two-word combinations
        for prefix in prefixes[:3]:
            for suffix in suffixes[:3]:
                variations.append(f"{prefix} {base_phrase} {suffix}")
        
        # Add some local variations
        for loc in local:
            variations.append(f"{base_phrase} {loc}")
        
        # Ensure uniqueness and limit
        seen = set()
        unique_variations = []
        for v in variations:
            if v not in seen:
                seen.add(v)
                unique_variations.append(v)
        
        return unique_variations[:80]  # Reasonable limit

class FreeTierSerpAPI:
    """Optimized SerpApi client for free tier usage"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.requests_today = 0
        self.daily_limit = 50  # Conservative estimate for free tier
        
    def get_serp_data(self, keyword: str, use_cache: bool = True) -> Dict:
        """Get SERP data with caching and rate limiting"""
        
        if use_cache:
            cache_key = hashlib.md5(f"serp_{keyword}".encode()).hexdigest()
            # Implementation would check cache here
        
        if self.requests_today >= self.daily_limit:
            print(f"âš ï¸ Daily SerpApi limit reached. Using simulated data for: {keyword}")
            return self._simulate_serp_data(keyword)
        
        try:
            params = {
                'engine': 'google',
                'q': keyword,
                'api_key': self.api_key,
                'num': 5,  # Reduced for free tier
                'timeout': 5000
            }
            
            response = requests.get('https://serpapi.com/search', params=params, timeout=10)
            self.requests_today += 1
            
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âš ï¸ SerpApi error for '{keyword}': {response.status_code}")
                return self._simulate_serp_data(keyword)
                
        except Exception as e:
            print(f"âš ï¸ SerpApi failed for '{keyword}': {e}")
            return self._simulate_serp_data(keyword)
    
    def _simulate_serp_data(self, keyword: str) -> Dict:
        """Intelligent simulation based on keyword characteristics"""
        word_count = len(keyword.split())
        
        # More realistic simulation
        if word_count == 1:
            # Short tail - high competition
            return {
                'organic_results': [{'link': f'https://example.com/{keyword}'} for _ in range(8)],
                'search_information': {'total_results': 10000000},
                'competition_level': 'high'
            }
        elif word_count == 2:
            # Medium tail
            return {
                'organic_results': [{'link': f'https://example.com/{keyword}'} for _ in range(5)],
                'search_information': {'total_results': 5000000},
                'competition_level': 'medium'
            }
        else:
            # Long tail - low competition
            return {
                'organic_results': [{'link': f'https://example.com/{keyword}'} for _ in range(3)],
                'search_information': {'total_results': 100000},
                'competition_level': 'low'
            }

# Main optimized agent
class FreeTierSEOAgent(OptimizedSEOAgent):
    def __init__(self):
        super().__init__()
        self.serp_client = FreeTierSerpAPI(self.serpapi_key) if self.serpapi_key else None
        
    def analyze_keywords(self, seed_keyword: str, top_n: int = 50) -> pd.DataFrame:
        """Optimized analysis pipeline for free tier"""
        print(f"ðŸ” Starting analysis for: {seed_keyword}")
        
        # Step 1: Generate keywords (cached when possible)
        print("ðŸ”„ Generating keyword variations...")
        raw_keywords = self.nim_client.generate_keywords(seed_keyword, 60)  # Reduced for speed
        unique_keywords = list(dict.fromkeys(raw_keywords))  # Fast deduplication
        
        print(f"ðŸ“Š Generated {len(unique_keywords)} unique keywords")
        
        # Step 2: Analyze top 30 keywords for speed
        keyword_metrics = []
        analysis_count = min(30, len(unique_keywords))
        
        for i, keyword in enumerate(unique_keywords[:analysis_count]):
            print(f"   Analyzing {i+1}/{analysis_count}: {keyword[:50]}...")
            metrics = self._get_keyword_metrics(keyword)
            keyword_metrics.append(metrics)
            
            # Small delay to be API-friendly
            time.sleep(0.1)
        
        # Step 3: Score and rank
        scored_metrics = self._score_keywords(keyword_metrics)
        
        # Convert to DataFrame
        df = pd.DataFrame(scored_metrics[:top_n])
        
        print(f"âœ… Analysis complete! Found {len(df)} high-opportunity keywords")
        return df
    
    def _get_keyword_metrics(self, keyword: str) -> Dict:
        """Get metrics for a single keyword"""
        if self.serp_client and self.serpapi_key != 'your_serpapi_key_here':
            serp_data = self.serp_client.get_serp_data(keyword)
            return self._analyze_serp_data(serp_data, keyword)
        else:
            return self._simulate_metrics(keyword)
    
    def _analyze_serp_data(self, serp_data: Dict, keyword: str) -> Dict:
        """Analyze SERP data with enhanced heuristics"""
        organic_results = serp_data.get('organic_results', [])
        total_results = serp_data.get('search_information', {}).get('total_results', 1000000)
        
        # Enhanced competition analysis
        strong_domains = ['wikipedia.org', 'amazon.com', 'youtube.com', 'forbes.com', 'medium.com']
        domain_authorities = []
        
        for result in organic_results[:5]:  # Only check top 5 for speed
            domain = result.get('link', '').split('/')[2] if '/' in result.get('link', '') else ''
            if any(sd in domain for sd in strong_domains):
                domain_authorities.append(1.0)
            else:
                domain_authorities.append(0.3)
        
        # Calculate competition score (0-1)
        authority_score = sum(domain_authorities) / len(domain_authorities) if domain_authorities else 0.5
        results_density = min(total_results / 1000000, 1.0)
        competition_score = (authority_score * 0.7) + (results_density * 0.3)
        
        # Intelligent volume estimation
        word_count = len(keyword.split())
        if word_count == 1:
            base_volume = 5000
        elif word_count == 2:
            base_volume = 2000
        else:
            base_volume = 500
        
        # Adjust based on keyword characteristics
        if any(word in keyword.lower() for word in ['how', 'what', 'why', 'tutorial', 'guide']):
            base_volume *= 1.2  # Informational queries often have higher volume
        
        volume = max(50, base_volume)
        
        return {
            'keyword': keyword,
            'estimated_volume': volume,
            'competition_score': round(competition_score, 3),
            'serp_results_count': total_results,
            'strong_domains_count': len([d for d in domain_authorities if d > 0.5]),
            'word_count': word_count,
            'data_source': 'serpapi'
        }
    
    def _simulate_metrics(self, keyword: str) -> Dict:
        """Enhanced simulation with better heuristics"""
        word_count = len(keyword.split())
        
        # More realistic volume simulation
        if word_count == 1:
            volume = 3000 + hash(keyword) % 2000  # Add some variation
            competition = 0.7 + (hash(keyword) % 300) / 1000
        elif word_count == 2:
            volume = 1000 + hash(keyword) % 1000
            competition = 0.4 + (hash(keyword) % 400) / 1000
        else:
            volume = 200 + hash(keyword) % 300
            competition = 0.2 + (hash(keyword) % 300) / 1000
        
        # Adjust for commercial intent
        if any(word in keyword.lower() for word in ['buy', 'price', 'cost', 'review']):
            volume *= 1.5
            competition *= 1.2
        
        return {
            'keyword': keyword,
            'estimated_volume': max(50, int(volume)),
            'competition_score': round(min(0.95, max(0.1, competition)), 3),
            'serp_results_count': 1000000,
            'strong_domains_count': 2 if word_count == 1 else 1,
            'word_count': word_count,
            'data_source': 'simulated'
        }
    
    def _score_keywords(self, keyword_metrics: List[Dict]) -> List[Dict]:
        """Score keywords with enhanced algorithm"""
        for metrics in keyword_metrics:
            # Enhanced scoring that considers keyword length and intent
            volume_score = min(metrics['estimated_volume'] / 5000, 1.0)
            competition_score = metrics['competition_score']
            
            # Bonus for long-tail keywords (better targeting)
            length_bonus = 0.1 if metrics['word_count'] >= 3 else 0
            
            # Penalty for high competition
            competition_penalty = competition_score * 0.4
            
            metrics['composite_score'] = round(
                (volume_score * 0.6) - competition_penalty + length_bonus, 
                4
            )
        
        return sorted(keyword_metrics, key=lambda x: x['composite_score'], reverse=True)

# Global instance
seo_agent = FreeTierSEOAgent()
